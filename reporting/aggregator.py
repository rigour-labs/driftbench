import os
import json
import glob
import click
from runner.engine import BenchmarkEngine, Task

@click.command()
@click.option('--results-dir', default='results', help='Directory where results are stored')
@click.option('--model', help='Model slug (if aggregating specific model runs)')
@click.option('--output', help='Output file (defaults to LEADERBOARD.md or <model>_REPORT.md)')
def main(results_dir, model, output):
    """Aggregates all results into a leaderboard."""
    search_path = results_dir
    if model:
        search_path = os.path.join(results_dir, model.replace("/", "_"))
        if not output:
            output = f"{model.replace('/', '_')}_REPORT.md"
    
    if not output:
        output = 'LEADERBOARD.md'
        
    os.makedirs(results_dir, exist_ok=True)
    
    result_files = glob.glob(f"{search_path}/*.json")
    
    stats = {
        "total_tasks": 0,
        "detected_drift": 0,
        "total_candidates": 0,
        "by_category": {}
    }
    
    rows = []
    
    for res_path in result_files:
        with open(res_path, 'r') as f:
            data = json.load(f)
            
        if isinstance(data, dict) and data.get("status") == "INVALID_TASK":
            continue

        # result can be a list (from evaluate_task) or a dict (from evaluate_patch)
        if isinstance(data, dict):
            # Single patch result (e.g. from harness)
            tasks_list = [data]
            task_id = os.path.basename(res_path).replace(".json", "")
            # We'd need to lookup category for better reporting, but for now we skip lookup
            category = "unknown"
        else:
            tasks_list = data
            task_id = os.path.basename(res_path).replace(".json", "")
            category = "multi" # Simplified

        stats["total_tasks"] += 1
        
        for cand in tasks_list:
            stats["total_candidates"] += 1
            cat = cand.get("drift_type", "unknown")
            if cat not in stats["by_category"]:
                stats["by_category"][cat] = {"total": 0, "detected": 0}
            
            stats["by_category"][cat]["total"] += 1
            if cand["detected"]:
                stats["detected_drift"] += 1
                stats["by_category"][cat]["detected"] += 1
        
        detected_count = sum(1 for c in tasks_list if c["detected"])
        total_count = len(tasks_list)
        ddr = (detected_count / total_count * 100) if total_count > 0 else 0
        rows.append(f"| {task_id} | {detected_count}/{total_count} | {ddr:.1f}% |")

    # Generate Markdown
    with open(output, 'w') as f:
        title = f"DriftBench Report: {model}" if model else "DriftBench Global Leaderboard ðŸŽï¸"
        f.write(f"# {title}\n\n")
        
        f.write("## Global Metrics\n\n")
        ddr_global = (stats["detected_drift"] / stats["total_candidates"] * 100) if stats["total_candidates"] > 0 else 0
        f.write(f"- **Total Tasks Evaluated**: {stats['total_tasks']}\n")
        f.write(f"- **Total Candidates/Variations**: {stats['total_candidates']}\n")
        f.write(f"- **Global DDR (Drift Detection Rate)**: **{ddr_global:.1f}%**\n")
        f.write(f"- **FPR (False Positive Rate)**: **0.0%** (Enforced by Golden Patch validation)\n\n")
        
        f.write("## Category Performance\n\n")
        f.write("| Category | Detected | Total | DDR |\n")
        f.write("| :--- | :--- | :--- | :--- |\n")
        for cat, s in sorted(stats["by_category"].items()):
            c_ddr = (s["detected"] / s["total"] * 100) if s["total"] > 0 else 0
            f.write(f"| {cat} | {s['detected']} | {s['total']} | {c_ddr:.1f}% |\n")
        
        f.write("\n## Task Breakdown\n\n")
        f.write("| Task ID | Detected/Total | DDR |\n")
        f.write("| :--- | :--- | :--- |\n")
        for row in sorted(rows):
            f.write(row + "\n")

    click.secho(f"âœ… Report generated: {output}", fg='green', bold=True)

if __name__ == "__main__":
    main()
